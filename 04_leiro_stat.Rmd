---
bibliography: references.bib
---

# Információ-visszakeresés és információkinyerés, szövegek reprezentálása a vektortérben, leíró statisztika

A szövegbányászati feladatok két altípusa a keresés és a rendszerezés. A keresés során olyan szövegeket keresünk, amelyekben egy adott kifejezés előfordul, a rendszerezés során pedig a szövegeket hasonlítjuk össze egymással és egy előre megadott, vagy egy előzetesen nem ismert kategóriarendszer csoportjaihoz soroljuk őket. Az információ-visszakeresés (information retrieval), ami például a webes keresőprogramok egyik jellemző tevékenysége, során a cél, hogy a korpuszból visszakeressük a kereső információigénye szempontjából releváns információkat, mely keresés alapulhat metaadatokon vagy teljes szöveges indexelésen. [@tikk2007:63; @russel2005a:742] Az információkinyerés (information extraction) esetén a cél, hogy a strukturálatlan szövegekből strukturált adatokat állítsunk elő. Azaz az információkinyerés során nem a felhasználó által keresett információt keressük meg és lokalizáljuk, hanem az adott kérdés szempontjából releváns információkat gyűjtjük ki a dokumentumokból. aAz információkinyerés alternatív megoldása segítségével már képesek lehetünk a kifejezések közötti kapcsolatok elemzésére, tendenciák és minták felismerésére és az információk összekapcsolás révén új információk létrehozására. Azaz a segítségével strukturálatlan szövegekből is előállíthatunk strukturált információkat [@kwartler2017; @schütze2008 ; @tikk2007a:63-81.]

## A szövegek reprezentálása a vektortérben - szózsák modell

A szövegbányászati vizsgálatok során folyó szövegek, azaz strukturálatlan vagy részben strukturált dokumentumok elemzésére kerül sor, melyekből a kutatási kérdéseink szempontjából releváns, látens összefüggéseket nyerünk ki, amelyek már strukturált szerkezetűek. A dokumentumok reprezentálásnak három legelterjedtebb módja a halmazelmélet alapú, az algebrai és a valószínűségi modell. A halmazelméleti modellek a dokumentumok hasonlóságát halmazelmélet, a valószínűségi modellek pedig feltételes valószínűségi becslés alapján határozzák meg. Az algebrai modellek a dokumentumokat vektorként vagy mátrixként ábrázolják és algebrai műveletek segítségével hasonlítják össze. A vektortérmodell sokdimenziós vektortérben ábrázolja a dokumentumokat, úgy hogy a dokumentumokat vektorokkal reprezentálja, a vektortér dimenziói pedig a dokumentumok összességében előforduló egyedi szavak. A modell alkalmazása során azok a dokumentumok hasonlítanak egymásra, amelyeknek a szókészlete átfedi egymást, és a hasonlóság mértéke az átfedéssel arányos. A vektortérmodellben a dokumentumgyűjteményt a szó-dokumentum mátrixszal (term-document matrix) reprezentáljuk, a mátrixban a sorok száma megegyezik az egyedi szavak számával, az oszlopokat pedig a dokumentumvektorok alkotják. Az egyedi szavak összességét szórátnak nevezzük. Mivel mátrixban az egyedi szavak száma általában igen nagy, ezért a mátrix hatékony kezeléséhez annak mérte különböző eljárásokkal csökkenthető. Fontos tudni, hogy a dokumentumok vektortér reprezentációjában a szavak szövegen belüli sorrendjére és pozíciójára vonatkozó információ nem található meg. [@russel2005b:742-744; @kwartler2017:20; @welbers2017] A vektortérmodellt szózsák (bag of words) modellnek is nevezzük, melynek segítségével a fent leírtak szerint az egyes szavak gyakoriságát vizsgálhatjuk meg egy adott korpuszon belül. A szó-dokumentum mátrix lehet egy egyszerű bináris mátrix, ami csak azt az információt tartalmazza, hogy egy adott szó előfordul-e egy adott dokumentumban. Míg az egyszerű bináris mátrixban ugyanakkora súlya van egy szónak ha egyszer és ha tízszer szerepel, készíthetünk olyan mátrixot is, ahol egy szónak annál nagyobb a súlya egy dokumentumban, minél többször fordul elő. Emellett a mártixot a dokumentumok hossza szerint is normálhatjuk. Az így súlyozott mátrixban nem azt vizsgáljuk, hogy egy szó hányszor egy dokumentumban, hanem ezt a számot viszonyítjuk az adott dokumentum szavainak a számához. A szógyakoriság (term frequency, TM) szerint súlyozott TD mátrixnál azt is figyelembe vesszük, hogy az adott szó hány dokumentumban szerepel. Minél több dokumentumban szerepel egy szó, annál kisebb a jelentősége. Ilyen szavak például a névelők, amelyek sok dokumentumban előfordulnak ugyan, de nem sok tartalmi jelentőséggel bírnak. Két szó közül általában az a fontosabb, amelyik koncentráltan, kevés dokumentumban, de azokon belül nagy gyakorisággal fordul elő. A dokumentum gyakorisági érték (document frequency, df) egy szó ritkaságát jellemzi egy korpuszon belül, azaz megadja, hogy mekkora megkülönböztető ereje van egy szónak a dokumentum tartalmára vonatkozóan. A súlyozási sémákban általában a dokumentum gyakorisági érték inverzével számolnak (inverse document frequency, idf) ez a leggyakrabban használt td-idf súlyozás (term frequency & inverse document frequency. Az így súlyozott TD mátrix egy-egy cellájában található érték azt mutatja, hogy egy adott szónak mekkora a jelentősége egy adott dokumentumban. A tf -idf súlyozás értéke tehát magas azon szavak esetén, amelyek az adott dokumentumban gyakran fordulnak elő, míg a teljes korpuszban ritkán, alacsonyabb azon szavak esetén, amelyek az adott dokumentumban ritkábban, vagy a korpuszban gyakrabban fordulnak elő és kicsi azon szavaknál, amelyek a korpusz lényegében összes dokumentumában előfordulnak [@tikk2007:33-37; @grimmer2013; @welbers2017]

## Leíró statisztika

Fejezetünkben nyolc véletlenszerűen kiválasztott magyar miniszterelnöki beszéd vizsgálatát végezzük el, amihez az alábbi csomagokat használjuk:

```{r message=FALSE, warning=FALSE}
library(readtext)
library(dplyr)
library(lubridate)
library(stringr)
library(ggplot2)
library(quanteda)
library(GGally)
library(tidytext)
library(tidyverse)
```

Első lépésben a már ismertetett módon a `readtext ()` segítségével beolvassuk a beszédek txt formátumú változatát, utf-8 karakterkódolással.

```{r message=FALSE, warning=FALSE}
texts <- readtext("mineln/*.txt", encoding = "UTF-8")
head(texts)
```

Ezt követően az "Adarkezelés R-ben" fejezetben ismertetett `mutate()` függvény használazával két csoportra osztjuk a beszédeket. Ehhez először a `string_extract()`segítségével meghatározzuk, hogy a kettéosztáshoz hasznáni kívánt új változó a doc_id legyen a `[^\\.]*` regex segítségével leválasztva arról a `.txt` kiterjesztést, majd a `str_sub()` függvénnyel megmondjuk, hogy a miniszterelnökök neve a `doc_id` háturló számított hatodik karakteréig tart. Ezután kialakítjuk a két csoportot, azaz az `if_else()` segítségével meghatározzuk, hogy ha "antall_jozsef", "boross_peter", "orban_viktor" beszédeiről van szó azokat a jobb csoportba tegye, a maradékot pedig a bal csoportba.

Ezt követően azt is meghatározzuk, hogy melyik beszédnek mi a dátuma. Ehhez szintén a `str_sub()` függvényt használjuk, majd a lubridate segítségével alakítjuk ki a kívánt dátumformátumot[^1].

[^1]: A \`lubridate\` használatának részletes leírása megtalálható az alábbi linken: <https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf>

Majd a `glimpse()` függvény segítsével megtekintjük, hogy milyen változtatásokat végeztünk az adattáblánkon. Láthatjuk, hogy míg korábban 8 dokumentumunk és 2 változónk volt, az átalakítás eredményeként a 8 dokumentum mellett már 5 változót találunk. Ezzel a lépéssel tehát kialakítottuk azokat a változókat, amelyekre az elemzés során szükségünk lesz.

```{r message=FALSE, warning=FALSE}

texts <- texts %>% 
    mutate(doc_id = str_extract(doc_id, "[^\\.]*"),
           mineln = str_sub(doc_id, end = -6),
           wing = if_else(mineln %in% c("antall_jozsef", "boross_peter", "orban_viktor"), "jobb", "bal"))

texts$year <- str_sub(texts$doc_id, start = -2) %>%
    str_c("-01-01") %>%
    lubridate::ymd() %>%
    lubridate::year()

glimpse(texts)

```

Ezt követően a további lépések elvégzéséhez létrehozzuk a `quanteda` korpuszt, majd a `summary()` függvény segítségével megtekinthetjük a korpusz alaovető satisztikai jellemzőit, láthatjuk például, hogy az egyes dokumentumok hány tokenből vagy mondatból állnak.

```{r message=FALSE, warning=FALSE}

corpus_mineln <- corpus(texts)

summary(corpus_mineln)

```

Mivel az elemzés során a korpuszon belül két csoportra osztva szeretnénk összehasonlításokat tenni, az alábbiakban két alkorpuszt alakítunk ki.

```{r message=FALSE, warning=FALSE}

mineln_jobb <- corpus_subset(corpus_mineln, mineln %in% c("antall_jozsef", "boross_peter", "orban_viktor"))

mineln_bal <- corpus_subset(corpus_mineln, mineln %in% c("horn_gyula", "gyurcsany_ferenc", "medgyessy_peter", "bajnai_gordon"))

summary(mineln_jobb)

summary (mineln_bal)
```

A korábban létrehozott "jobb" és "bal" változó segítségével nem csak az egyes dokumentumokat, hanem a két csoportba sorolt beszédket is összehaonlíthatjuk egymással.

```{r message=FALSE, warning=FALSE}
summary(corpus_mineln) %>% 
    group_by(wing) %>% 
    summarise(mean_wordcount = mean(Tokens), std_dev = sd(Tokens), min_wordc = min(Tokens), max_wordc = max(Tokens))
```

A `textstat_collocations()` függvény segítségével együtt előforduló szókapcsolatokat kereshetünk. A függvény argumentumai közül a `size` a szókapcsolatok hossza, a `min_count` pedig a minimális előfordulásuk száma. Miután a szókapcsolatokat megkeresük a korábban már megismert `head()` függvény segítségével megnézhetünk közülük tetszőleges számút[^2].

[^2]: A lambda leírása megtalálható: <https://quanteda.io/reference/textstat_collocations.html>

```{r message=FALSE, warning=FALSE}

corpus_mineln %>% 
    textstat_collocations(size = 3,
  min_count = 6) %>% 
  
    head(n = 10)
```

A szókapcsolatok listázásánál is láthattuk, hogy a korpuszunk még minden szót tartalmaz, ezért találtunk például "hogy ez a" összetételt. A következőkben eltávolítjuk az ilyen funkció nélküli stopszavakat a korpuszból, amihez saját stopszólistát használunk. Először beolvassuk és egy `custom_stopwords` nevű objektumban tároljuk a stopszavakat, majd a `tokens()` függvény segítségével tokenizáljuk a korpuszt és a `tokens_select()` használatával eltávolítjuk a stopszavakat.

Ha ezután újra megnézzük a kollokációkat, jól látható a stopszavak eltávolításának eredménye:

```{r message=FALSE, warning=FALSE}
custom_stopwords <- readLines("stopwords.txt", encoding = "UTF-8")

corpus_mineln %>% 
    tokens() %>% 
    tokens_select(pattern = custom_stopwords, selection = "remove") %>%
    textstat_collocations(size = 3,
  min_count = 6)%>%  
  
  head(n = 10)

```

A korpuszon további elemzése előtt fontos, hogy ne csak a stopszavakat távolítsuk el, hanem az egyéb alapvető szövegtisztító lépéseket is elvégezzük. Azaz a `tokens_select()` segítségével eltávolítsuk a számokat, a központozást, az elválasztó karaktereket, mint például a szóközöket, tabulátorokat, sortöréseket. Ezután a `tokens_ngrams()` segítségével `ngarm`-okat hozunk létre a tokenekből, majd kialakítjuk a dokumentumk kifejezés mátrixot (`dfm`) és elvégezzük a `tf-idf` szerinti súlyozást. A `dfm_tfidf()`függvény kiszámolja a dokumentum gyakoriság inzverz súlyozását. A függvény alapértelmezés szerint a normalizált kifejezések gyakoriságát használja a dokumentumon belüli relatív kifejezés gyakoriság helyett, ezt írjuk felül a `schem__tf = "prop"` használatával. Végül a `textstat_frequency()` segítségével gyakorisági statisztikát készíthetünk a korábban meghatározott (példánkban két és három tagú) `ngram`-okról.

```{r message=FALSE, warning=FALSE}

corpus_mineln %>% 
    tokens(remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE) %>% 
    tokens_select(pattern = custom_stopwords, selection = "remove") %>%
    tokens_ngrams(n = 2:3) %>% 
    dfm() %>% 
    dfm_tfidf(scheme_tf = "prop") %>% 
    textstat_frequency(n = 10, force = TRUE)

```

## A szövegek lexikai diverzitása

Az alábbiakban a korpuszunkat alkotó szövegek lexikai diverzitásat vizsgáljuk. Ehhez a `quanteda` csomag `textstat_lexdiv()`függvényét használjuk. Mivel ez a függvény `dfm`-et elemez, először a `corpus_mineln` nevű korpuszunkból létrehozzuk a `mineln_dfm` nevű `dfm`-et, amelyen elvégezzük a korábban már megismert alapvető tisztító lépéseket. A `textstat_lexdiv()` függvény eredménye szintén egy `dfm`, így azt `arrange()`parancs argumentumában a `desc` megadásával csökkenő sorba is rendezhetjük. A`textstat_lexdiv()` különböző indexek segítségével számítja ki a szövegek lexikai különbözöségét, példánkban a `CTTR` indexet használjuk[^3].

[^3]: A különböző indexek leírása megtalálható az alábbi linken: <https://quanteda.io/reference/textstat_lexdiv.html>

```{r message=FALSE, warning=FALSE}
mineln_dfm <- corpus_mineln %>% 
    tokens(remove_punct = TRUE, remove_separators = TRUE, remove_hyphens = TRUE) %>% 
    dfm(remove = custom_stopwords)

mineln_dfm %>% 
    textstat_lexdiv(measure = "CTTR") %>% 
    arrange(desc(CTTR))

```

A kiszámolt értéket hozzáadhatjuk a `dfm`-hez is.

```{r message=FALSE, warning=FALSE}
dfm_lexdiv <- mineln_dfm

cttr_score <- unlist(textstat_lexdiv(dfm_lexdiv, measure = "CTTR")[,2])

docvars(dfm_lexdiv, "cttr") <- cttr_score

docvars(dfm_lexdiv)

```

A fenti elemzést elvégezhetjük úgy is, hogy valamennyi indexálást egyben megkapjuk. Ehhez a `textstat_lexdiv()` függvény argumentumába a `measure = "all"` kifejezést kell megadnunk.

```{r message=FALSE, warning=FALSE}
mineln_dfm %>% 
    textstat_lexdiv(measure = "all")
```

Ha pedig arra vagyunk kíváncsiak, hogy a kapott értékek hogyan viszonyulanak egymáshoz, azt a `cor()` függvény segítésével számolhatjuk ki.

```{r message=FALSE, warning=FALSE}
div_df <- mineln_dfm %>% 
    textstat_lexdiv(measure = "all")


cor(div_df[,2:13])
```

A kapott értékeket a `ggcorr()` függvény segítségével ábrázolhatjuk is. Ha a függvény argumentumában a `label = TRUE` szerepel, a kapott ábrán a kiszámított értékek is szerepelnek.

```{r message=FALSE, warning=FALSE}
ggcorr(div_df[,2:13], label = TRUE)

```

Ezt követően azt is megvizsgálhatjuk, hogy a korpusz szövei mennyire könnyen olvashatóak. Ehhez a `Flesch.Kincaid` pontszámot használjuk, ami a szavak és mondatok hossza alapján határozza meg a szöveg olvashatóságát. Ehhez `textstat_readability()` függvényt használjuk, mely a korpuszunkat elemzi.

```{r message=FALSE, warning=FALSE}
corpus_mineln %>% 
    textstat_readability(measure = "Flesch.Kincaid")
```

Ezután a kiszámított értékkel kiegészítjük a korpuszt

```{r message=FALSE, warning=FALSE}
docvars(corpus_mineln, "f_k") <- textstat_readability(corpus_mineln, measure = "Flesch.Kincaid")[,2]

docvars(corpus_mineln)

```

```{r message=FALSE, warning=FALSE}
docvars(corpus_mineln, "f_k") <- textstat_readability(corpus_mineln, measure = "Flesch.Kincaid")[,2]

docvars(corpus_mineln)
```

Majd a `ggplot2` segítségével vizualizálhatjuk az eredményt. Ehhez az olvashatósági pontszámmal kiegészített korpuszból data fram-et alakítunk ki, majd beállítjuk az ábrázolás paramétereit. Azaz, hogy a két tengelyen az év illetve az olvashatósági pontszám szerepeljen, a színezés különböztesse meg a jobb és a bal oldalt, az egyes dokumentumokat ponttal jelöljük, a jobb és bal oldali beszédeket vonallal kötjük össze, az ábrára fekete színnel felíratjuk a miniszterelnökök nevét, valamint, hogy az x tengely beosztása az egyes beszédek dátumához igazodjon. A `theme_minimal()` függvénnyel pedig azt határozzuk meg, hgy mindez fehér hátteret kapjon.

```{r message=FALSE, warning=FALSE}
corpus_df <- docvars(corpus_mineln)

ggplot(corpus_df, aes(year, f_k, color = wing)) +
    geom_point(size = 2) +
    geom_line(aes(linetype = wing), size = 1) +
    geom_text(aes(label = mineln), color = "black", nudge_y = 0.15) +
    scale_x_continuous(breaks = corpus_df$year) +
    theme_minimal()


```

## Összehasonlítás[^4]

[^4]: [@schütze2008a:294-307.]

A fentiekben láthattuk az eltéréseket a jobb és bal oldali beszédeken belül, sőt ugyanahhoz a miniszterelnökhöz tartozó két beszéd között is. A következőkben `textstat_dist()` és `textstat_simil()` függvények segítségével megvizsgáljuk, valójában mennyire hasonlítanak vagy különböznek ezek a beszédek. Mindkét függvény bemenete `dmf`, melyből először egy súlyozott `dfm`-et készítünk, majd elvégezzük az összehasonlítást először a `jaccard`-féle hasonlóság alapján.

```{r message=FALSE, warning=FALSE}


mineln_dfm %>% 
    dfm_weight("prop") %>% 
    textstat_simil(margin = "documents", method = "jaccard") 
```

Majd a `textstat_dist()` függvény segítségével kiszámoljuk a dokumentumok egymástól való különbözőségét.

```{r message=FALSE, warning=FALSE}
mineln_dfm %>%
    textstat_dist(margin = "documents", method = "euclidean")
```

Ezután vizualizálhatjuk is a dokumentumok egymástól való távolságát egy olyan dendogram segítségével, amely megmutatja nekünk a lehetséges dokumentumpárokat.

```{r message=FALSE, warning=FALSE}
dist <- mineln_dfm %>%
    textstat_dist(margin = "documents", method = "euclidean")

plot(hclust(as.dist(dist)))

```

```{r message=FALSE, warning=FALSE}
mineln_dfm %>% 
    textstat_simil(y = mineln_dfm[, c("kormány")], margin = "features", method = "correlation") %>% 
    head(n = 10)
```

Arra is van lehetőségünk, hogy a két alkorpuszt hasonlítsuk össze egymással. Ehhez a `textstat_keyness()` függvényt haszáljuk, melynek a bemenete a `dfm`. A függvény argumentumában a `target =` után kell szerepletnük, hogy mely alkorpusz a viszonyítási alap. Az összehasonlítás eredményét a 'textplot_keyness()\` függvény segítségével ábrázolhatjuk, ami megjeleníti a két alkorpusz leggyakoribb kifejezéseit.

```{r message=FALSE, warning=FALSE}
dfm <- dfm(corpus_mineln, groups = "wing", remove = custom_stopwords,
           remove_punct = TRUE)

result_keyness <- textstat_keyness(dfm, target = "jobb")

textplot_keyness(result_keyness) 

```

Ha az egyes miniszterelnökök beszédeinek leggyakoribb kifejezéseit szeretnénk összehasonlítani, azt a `textstat_frequency()` függvény segítségével tehetjük meg, melynek bemnete a megtisztított és súlyozott `dfm`. Az összehasonlítás eredményét pedig a `ggpolt2` segítségével ábrázolhatjuk is.

```{r message=FALSE, warning=FALSE}
dfm_weight <- corpus_mineln %>%
  dfm(remove = custom_stopwords,  tolower = TRUE, remove_punct = TRUE, stem = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>%
  dfm_weight(scheme = "prop")

freq_weight <- textstat_frequency(dfm_weight, n = 15, groups = "mineln")

ggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +
  geom_point() +
  facet_wrap(~ group, scales = "free") +
  coord_flip() +
  scale_x_continuous(breaks = nrow(freq_weight):1,
                     labels = freq_weight$feature) +
  labs(x = NULL, y = "Relative frequency")

```

## Kifejezések kontextusba helyezése

Arra is lehetőségünk van, hogy egyes kulcszavakat a korpuszon belül szövekörnyezetükben vizsgáljunk meg. Ehhez a `kwick()` függvényt használjuk, az argumentumok között a `pattern =` kifejezés után megadva azt a szót, amelyet vizsgálni szeretnénk, a 'window =' után pedig megadhatjuk hogy az adott szó hány szavas környezetére vagyunk kíváncsiak.

```{r message=FALSE, warning=FALSE}

kwic(corpus_mineln, pattern = "válság*",valuetype = "glob", window = 3, case_insensitive = TRUE) %>% 
  
    head(20)

```
